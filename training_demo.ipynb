{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "654150bd-6364-4759-89ca-0b553c2c6287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_built():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b7aae64-5cff-4aa6-805a-3b787b372d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from notebook.mnist_dataset import MnistVideoDataset\n",
    "\n",
    "image_size = 64\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "\n",
    "dataset = MnistVideoDataset(image_size=image_size)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eba46f1-6970-4d8d-b404-556c20348851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor Writer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "def create_tensorboard_writer(exp_dir):\n",
    "    tensorboard_dir = f\"{exp_dir}/tensorboard\"\n",
    "    os.makedirs(tensorboard_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(tensorboard_dir)\n",
    "    return writer\n",
    "\n",
    "writer = create_tensorboard_writer(\"local/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a882868-6512-4f5c-a932-b0787670201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook.vae import VideoAutoencoderKL\n",
    "\n",
    "vae = VideoAutoencoderKL(from_pretrained = \"stabilityai/sd-vae-ft-ema\", device=device)\n",
    "latent_size = vae.get_latent_size((3, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef4f575-72d3-42f8-85fd-51755b710f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notebook.t5 import T5Encoder\n",
    "\n",
    "# text_encoder = T5Encoder(from_pretrained='DeepFloyd/t5-v1_1-xxl', shardformer=False, model_max_length=1, device=device)\n",
    "\n",
    "from notebook.t5 import MNistEncoder\n",
    "text_encoder = MNistEncoder(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a2abe69-2c02-4997-8986-3bfe101e1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook.stdit import STDiT\n",
    "\n",
    "model = STDiT(\n",
    "    input_size=latent_size,\n",
    "    in_channels=vae.out_channels,\n",
    "    caption_channels=text_encoder.output_dim,\n",
    "    model_max_length=text_encoder.model_max_length,\n",
    "    dtype = \"bf16\",\n",
    "\n",
    "    depth=16,\n",
    "    hidden_size=32,\n",
    "    patch_size=(1, 2, 2),\n",
    "    num_heads=16\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926241e9-c31a-4c64-8def-9d2470c4785c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358144, 358144)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params = 0\n",
    "num_params_trainable = 0\n",
    "for p in model.parameters():\n",
    "    num_params += p.numel()\n",
    "    if p.requires_grad:\n",
    "        num_params_trainable += p.numel()\n",
    "num_params, num_params_trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e42431e-8803-4543-8854-622ec911fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebook.iddpm import IDDPM\n",
    "\n",
    "scheduler = IDDPM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7fbf8b37-0cd7-4e02-aa1e-2a640fb03092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming `model` is your model and `cfg.lr` is the learning rate from your configuration\n",
    "optimizer = optim.Adam(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4, weight_decay=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e79340a7-a28c-4859-b803-8ac78479d4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                                      | 0/60000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Diffusion\u001b[39;00m\n\u001b[1;32m     39\u001b[0m t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, scheduler\u001b[38;5;241m.\u001b[39mnum_timesteps, (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],), device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 40\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Backward & update\u001b[39;00m\n\u001b[1;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/workspace/personal/Open-Sora/notebook/iddpm/respace.py:97\u001b[0m, in \u001b[0;36mSpacedDiffusion.training_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_losses\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# pylint: disable=signature-differs\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workspace/personal/Open-Sora/notebook/iddpm/gaussian_diffusion.py:697\u001b[0m, in \u001b[0;36mGaussianDiffusion.training_losses\u001b[0;34m(self, model, x_start, t, model_kwargs, noise)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m noise \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    696\u001b[0m     noise \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrandn_like(x_start)\n\u001b[0;32m--> 697\u001b[0m x_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m terms \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m LossType\u001b[38;5;241m.\u001b[39mKL \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_type \u001b[38;5;241m==\u001b[39m LossType\u001b[38;5;241m.\u001b[39mRESCALED_KL:\n",
      "File \u001b[0;32m~/workspace/personal/Open-Sora/notebook/iddpm/gaussian_diffusion.py:218\u001b[0m, in \u001b[0;36mGaussianDiffusion.q_sample\u001b[0;34m(self, x_start, t, noise)\u001b[0m\n\u001b[1;32m    215\u001b[0m     noise \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrandn_like(x_start)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m noise\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m x_start\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 218\u001b[0m     \u001b[43m_extract_into_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt_alphas_cumprod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_start\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m x_start\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;241m+\u001b[39m _extract_into_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqrt_one_minus_alphas_cumprod, t, x_start\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m*\u001b[39m noise\n\u001b[1;32m    220\u001b[0m )\n",
      "File \u001b[0;32m~/workspace/personal/Open-Sora/notebook/iddpm/gaussian_diffusion.py:832\u001b[0m, in \u001b[0;36m_extract_into_tensor\u001b[0;34m(arr, timesteps, broadcast_shape)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_into_tensor\u001b[39m(arr, timesteps, broadcast_shape):\n\u001b[1;32m    824\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;124;03m    Extract values from a 1-D numpy array for a batch of indices.\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;124;03m    :param arr: the 1-D numpy array.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;124;03m    :return: a tensor of shape [batch_size, 1, ...] where the shape has K dims.\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 832\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m[timesteps]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(broadcast_shape):\n\u001b[1;32m    834\u001b[0m         res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "dtype = torch.float32\n",
    "start_epoch = start_step = log_step = sampler_start_idx = 0\n",
    "running_loss = 0.0\n",
    "n_epoch = 100\n",
    "num_steps_per_epoch = len(dataloader)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def all_reduce_mean(tensor: torch.Tensor):\n",
    "    dist.all_reduce(tensor=tensor, op=dist.ReduceOp.SUM)\n",
    "    tensor.div_(dist.get_world_size())\n",
    "    return tensor\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    logger.info(f\"Beginning epoch {epoch}...\")\n",
    "\n",
    "    with tqdm(\n",
    "        range(start_step, num_steps_per_epoch),\n",
    "        desc=f\"Epoch {epoch}\",\n",
    "        total=num_steps_per_epoch,\n",
    "        initial=start_step,\n",
    "    ) as pbar:\n",
    "        for step in pbar:\n",
    "            batch = next(dataloader_iter)\n",
    "            x = batch[\"video\"].to(device, dtype)  # [B, C, T, H, W]\n",
    "            y = batch[\"text\"]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Prepare visual inputs\n",
    "                x = vae.encode(x)  # [B, C, T, H/P, W/P]\n",
    "                # Prepare text inputs\n",
    "                model_args = text_encoder.encode(y)\n",
    "\n",
    "            # Diffusion\n",
    "            t = torch.randint(0, scheduler.num_timesteps, (x.shape[0],), device=device)\n",
    "            loss_dict = scheduler.training_losses(model, x, t, model_args)\n",
    "\n",
    "            # Backward & update\n",
    "            loss = loss_dict[\"loss\"].mean()\n",
    "            booster.backward(loss=loss, optimizer=optimizer)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Update EMA\n",
    "            # update_ema(ema, model.module, optimizer=optimizer)\n",
    "\n",
    "            # Log loss values:\n",
    "            all_reduce_mean(loss)\n",
    "            running_loss += loss.item()\n",
    "            global_step = epoch * num_steps_per_epoch + step\n",
    "            log_step += 1\n",
    "\n",
    "            avg_loss = running_loss / log_step\n",
    "            pbar.set_postfix({\"loss\": avg_loss, \"step\": step, \"global_step\": global_step})\n",
    "            running_loss = 0\n",
    "            log_step = 0\n",
    "            writer.add_scalar(\"loss\", loss.item(), global_step)\n",
    "\n",
    "            # # Save checkpoint\n",
    "            # if (global_step + 1) % 100 == 0:\n",
    "            #     save(\n",
    "            #         booster,\n",
    "            #         model,\n",
    "            #         ema,\n",
    "            #         optimizer,\n",
    "            #         lr_scheduler,\n",
    "            #         epoch,\n",
    "            #         step + 1,\n",
    "            #         global_step + 1,\n",
    "            #         cfg.batch_size,\n",
    "            #         coordinator,\n",
    "            #         exp_dir,\n",
    "            #         ema_shape_dict,\n",
    "            #     )\n",
    "            #     logger.info(\n",
    "            #         f\"Saved checkpoint at epoch {epoch} step {step + 1} global_step {global_step + 1} to {exp_dir}\"\n",
    "            #     )\n",
    "\n",
    "    start_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac9742b-064d-4898-b5c5-8fa044ec438c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
